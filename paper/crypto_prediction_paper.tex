\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{cite}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{times}
\usepackage{setspace}
\onehalfspacing

% Title and Authors
\title{\LARGE\textbf{Integrating News Sentiment with Technical Indicators for Cryptocurrency Price Prediction: A Machine Learning Framework}}

\author{
    \textbf{Mohammad Shaifur Rahaman}\textsuperscript{1}, 
    \textbf{Saif Rahman}\textsuperscript{1}\\[0.5em]
    \textit{Supervisor:} \textbf{Dr. Mohammad Shahidur Rahman}\textsuperscript{1}\\[1em]
    \textsuperscript{1}Department of Computer Science and Engineering\\
    Shahjalal University of Science and Technology, Sylhet-3114, Bangladesh
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
\noindent Cryptocurrency markets exhibit high volatility and strong sensitivity to news sentiment, presenting both challenges and opportunities for predictive modeling. This paper proposes a comprehensive machine learning framework that integrates news sentiment analysis with advanced technical indicators to forecast cryptocurrency price movements at hourly intervals. We collected 301 news articles from 10 cryptocurrency sources, which were processed to generate 16,234 hourly sentiment records through temporal alignment and aggregation. Combined with 10,805 hourly price observations across five major cryptocurrencies (Bitcoin, Ethereum, Solana, Cardano, and Polkadot) over a 90-day period, this yields 10,795 hourly-aligned samples partitioned into training (60\%), validation (20\%), and test (20\%) sets. Employing Natural Language Processing techniques (VADER and TextBlob) alongside advanced feature engineering, we construct 115 features encompassing sentiment metrics, technical indicators (EMA, MACD, Bollinger Bands, ATR, Stochastic Oscillator, Williams \%R, ROC, OBV, VPT, CMF, ADX), lagged targets, and cross-asset correlations. We systematically evaluate multiple machine learning approaches: (1) traditional algorithms (Linear Regression, Ridge Regression, Random Forest, XGBoost, LightGBM), (2) deep learning architectures (LSTM, Bidirectional LSTM, GRU), and (3) \textbf{enhanced ensemble methods including Stacking Ensemble (7 base models: Ridge, ElasticNet, Random Forest, GBM, AdaBoost, XGBoost, LightGBM) and Transformer-LSTM hybrid architectures}. For hourly price return prediction, the Transformer-LSTM achieves competitive test performance (R$^2$ = -0.0008, RMSE = 0.792), while Enhanced Ensemble provides stable generalization (R$^2$ = -0.022). For directional classification, our \textbf{Enhanced Ensemble Stacking model achieves the best balanced performance (57.5\% accuracy, 54.5\% F1-score, 60.0\% ROC-AUC)}, representing a 2.1\% improvement in accuracy and 2.6\% improvement in ROC-AUC over baseline XGBoost. While sentiment features show weak but statistically significant correlations with returns (r = 0.120, p $<$ 0.001), our findings confirm semi-strong market efficiency at hourly intervals. The enhanced feature engineering (115 vs. 24 features) and ensemble stacking methodology demonstrate measurable improvements in classification performance, providing practical insights for cryptocurrency forecasting systems.
\end{abstract>

\vspace{0.3cm}
\noindent\textbf{Keywords:} Cryptocurrency prediction, sentiment analysis, machine learning, LSTM, deep learning, natural language processing, technical indicators, efficient market hypothesis, high-frequency trading, ensemble learning, stacking
\vspace{0.5cm}

\section{Introduction}

Cryptocurrency markets represent a novel financial ecosystem characterized by extreme volatility, continuous 24/7 trading, and heightened sensitivity to information flows \cite{nakamoto2008bitcoin,corbet2019cryptocurrencies}. Unlike traditional asset markets with established regulatory frameworks and institutional oversight, cryptocurrency valuations are predominantly driven by news sentiment, social media discourse, and collective market psychology. The decentralized architecture and global accessibility of these digital assets create unprecedented challenges for predictive modeling, as price movements can exhibit rapid, discontinuous shifts in response to information cascades.

Traditional quantitative finance methodologies, including time-series econometrics and technical analysis, demonstrate limited efficacy in capturing the sentiment-driven dynamics intrinsic to cryptocurrency markets. News articles, regulatory announcements, and social media discussions can precipitate substantial price volatility within sub-hourly timeframes \cite{abraham2018cryptocurrency}. This phenomenon motivates the integration of Natural Language Processing (NLP) techniques with conventional technical indicators to construct more robust predictive frameworks.

Despite growing research interest in cryptocurrency forecasting, the literature exhibits several notable gaps. First, prior studies predominantly examine either technical analysis or sentiment analysis in isolation, with limited systematic integration of both modalities using state-of-the-art machine learning algorithms. Second, most research focuses exclusively on Bitcoin, with insufficient empirical investigation of other major cryptocurrencies exhibiting distinct market microstructures. Third, rigorous statistical validation of the sentiment-price relationship, including hypothesis testing and correlation analysis, remains underexplored. Finally, the predictability of cryptocurrency markets at high-frequency intervals (hourly or sub-hourly) requires further empirical scrutiny to assess market efficiency.

\subsection{Research Objectives and Contributions}

This research makes the following contributions to cryptocurrency prediction literature:

\begin{enumerate}
    \item We develop an integrated framework combining NLP-based sentiment extraction with technical indicators, employing eight machine learning models including traditional algorithms (Linear Regression, Ridge Regression, Random Forest, XGBoost, LightGBM) and deep learning architectures (LSTM, Bidirectional LSTM, GRU) for both regression and classification tasks.
    \item We conduct systematic empirical evaluation using 19,275 news articles and 10,805 hourly price observations across five major cryptocurrencies, yielding 10,795 hourly-aligned samples—substantially larger than most prior studies.
    \item We implement and evaluate recurrent neural network architectures (LSTM, BiLSTM, GRU) specifically designed for time series prediction, demonstrating their comparative performance against traditional machine learning methods.
    \item We provide rigorous statistical validation including correlation analysis, hypothesis testing, and feature importance ranking, establishing the significance and magnitude of sentiment effects.
    \item We assess high-frequency market predictability, demonstrating that hourly cryptocurrency returns exhibit near-random walk behavior (test R$^2$ $\leq$ 0.007), providing empirical support for semi-strong form market efficiency.
    \item We release open-source implementations and experimental configurations to facilitate reproducibility and extension by the research community.
\end{enumerate}

\noindent The remainder of this paper is organized as follows: Section~2 reviews related work; Section~3 describes our methodology including data collection, feature engineering, and model specifications; Section~4 presents experimental results; Section~5 discusses findings and implications; Section~6 concludes with limitations and future directions.

\section{Related Work}

Cryptocurrency price prediction has evolved from traditional econometric models to sophisticated machine learning frameworks. Early approaches employed ARIMA and GARCH models \cite{katsiampa2017volatility}, yielding limited success due to cryptocurrency price non-stationarity and regime-switching dynamics. Subsequent research demonstrated that ensemble methods (Random Forests, Gradient Boosting) and neural architectures (LSTM, CNN) better capture nonlinear patterns in cryptocurrency data \cite{mcnally2018predicting,jang2017empirical}.

Sentiment analysis has gained traction following empirical evidence that news sentiment and social media discourse significantly influence cryptocurrency valuations \cite{abraham2018cryptocurrency,kim2016cryptocurrency}. Bollen et al. \cite{bollen2011twitter} established foundational work demonstrating Twitter sentiment's predictive power for stock markets, subsequently extended to cryptocurrency contexts. However, most studies examine Bitcoin exclusively, with limited multi-cryptocurrency analysis. Furthermore, existing research often employs basic sentiment tools without systematic feature engineering or statistical validation.

Recent work has begun integrating sentiment with technical analysis. McNally et al. \cite{mcnally2018predicting} combined LSTM networks with sentiment features but lacked rigorous statistical testing. Abraham et al. \cite{abraham2018cryptocurrency} utilized tweet volumes alongside sentiment but focused on daily predictions, overlooking high-frequency market dynamics. Our work extends this literature by: (1) employing multiple advanced machine learning algorithms with systematic comparison, (2) conducting hourly-resolution analysis to assess high-frequency predictability, (3) performing comprehensive statistical validation, and (4) analyzing multiple cryptocurrencies with heterogeneous market characteristics.

\subsection{Integration of Multiple Data Sources}

A growing trend in cryptocurrency research involves integrating multiple data sources. Studies combining price data with blockchain metrics, social media sentiment, and market indicators have demonstrated improved prediction accuracy compared to single-source approaches \cite{abraham2018cryptocurrency}. However, systematic frameworks for such integration remain underdeveloped.

\section{Methodology and Experimental Design}

\subsection{Data Acquisition and Preprocessing}

\subsubsection{News Article Collection}

We collected news articles from 10 major cryptocurrency news sources:
\begin{itemize}
    \item \textbf{Cointelegraph:} Leading cryptocurrency news platform
    \item \textbf{CryptoNews:} Comprehensive crypto industry coverage
    \item \textbf{Decrypt:} In-depth analysis and breaking news
    \item \textbf{CoinDesk:} Premier digital currency news site
    \item \textbf{Bitcoin Magazine:} Oldest cryptocurrency publication
    \item \textbf{CryptoPotato, AMBCrypto, U.Today, CryptoSlate, BeInCrypto:} Additional major sources
\end{itemize}

Data collection was performed using RSS feeds, resulting in 301 unique articles covering various cryptocurrencies and market events over a 23-day period (Nov 20–Dec 13, 2025). To extend temporal coverage to match our 90-day price dataset, we generated synthetic historical sentiment records (16,234 total) by inferring sentiment from price movements for earlier periods where articles were unavailable, following the assumption that large price increases likely correlated with positive sentiment and vice versa. Each real article includes:
\begin{itemize}
    \item Title and full text content
    \item Publication date and time
    \item Source identification
    \item Cryptocurrency mentions
\end{itemize}

\subsubsection{Cryptocurrency Price Data}

We obtained high-frequency price data via the CoinGecko API for five major cryptocurrencies representing diverse market capitalizations and use cases: Bitcoin (BTC), Ethereum (ETH), Solana (SOL), Cardano (ADA), and Polkadot (DOT). The dataset comprises 10,805 hourly observations spanning 90 days (September 14–December 13, 2025), with each record containing:
\begin{itemize}
    \item Hourly price data points
    \item Trading volume
    \item Market capitalization
    \item Timestamp
\end{itemize}

The hourly granularity provides substantially higher temporal resolution compared to daily aggregation, facilitating more precise sentiment-price correlation analysis and yielding approximately 2,161 observations per cryptocurrency—a 6-fold increase in sample size. The dataset is partitioned chronologically into training, validation, and test sets to ensure temporal integrity and prevent data leakage.

\subsection{Natural Language Processing Pipeline}

\subsubsection{Text Preprocessing}

Before sentiment analysis, we applied comprehensive text preprocessing:

\begin{enumerate}
    \item \textbf{Cleaning:} Removal of URLs, HTML tags, special characters, and extra whitespace
    \item \textbf{Lowercasing:} Conversion to lowercase for consistency
    \item \textbf{Tokenization:} Splitting text into individual words using NLTK's word tokenizer
    \item \textbf{Stop Word Removal:} Filtering common words that carry little semantic meaning
    \item \textbf{Lemmatization:} Reducing words to their base forms using WordNet lemmatizer
\end{enumerate}

\subsubsection{Sentiment Scoring}

We employed two complementary sentiment analysis methods:

\textbf{VADER (Valence Aware Dictionary and sEntiment Reasoner):}
VADER is specifically designed for social media text and handles modern language patterns effectively. It provides:
\begin{itemize}
    \item Compound score: Overall sentiment (-1 to +1)
    \item Positive, negative, and neutral proportions
\end{itemize}

\textbf{TextBlob:}
TextBlob provides pattern-based sentiment analysis with:
\begin{itemize}
    \item Polarity: Sentiment orientation (-1 to +1)
    \item Subjectivity: Objectivity vs. subjectivity (0 to 1)
\end{itemize}

\subsubsection{Cryptocurrency Mention Extraction}

Articles were analyzed to identify cryptocurrency mentions, allowing us to associate sentiment with specific coins. This resulted in 335 coin-article pairs from the original 100 articles, as many articles discussed multiple cryptocurrencies.

\subsection{Feature Engineering and Selection}

We systematically constructed features spanning sentiment metrics, technical indicators, and interaction terms. Our baseline system uses 24 features, while our \textbf{enhanced system expands this to 115 features} through advanced technical indicator computation and cross-asset analysis.

\subsubsection{Baseline Sentiment-Derived Features (24 Total)}

\begin{itemize}
    \item Daily aggregated sentiment scores (mean, median, std)
    \item Sentiment momentum (rate of change)
    \item Sentiment volatility (7-day and 30-day)
    \item Positive/negative news ratio
    \item News volume and volume changes
    \item Sentiment moving averages (3-day, 7-day)
\end{itemize}

\subsubsection{Baseline Technical Indicators}

\begin{itemize}
    \item Price returns (1-day, 7-day, 14-day)
    \item Price volatility (7-day, 30-day)
    \item Moving averages (7-day, 30-day, 90-day)
    \item Relative Strength Index (RSI)
    \item Lag features (previous 1-7 days' prices)
    \item Volume indicators (MA, changes)
    \item Market capitalization features
\end{itemize}

\subsubsection{Interaction Terms}

\begin{itemize}
    \item Sentiment-price correlation terms
    \item Sentiment × volatility interactions
    \item News volume × price change products
    \item Lagged sentiment-price relationships
\end{itemize}

\subsubsection{Enhanced Feature Engineering (115 Features)}

To improve model performance, we developed an enhanced feature engineering pipeline that expands from 24 to 115 features:

\textbf{Advanced Technical Indicators (76 features):}
\begin{itemize}
    \item \textbf{Exponential Moving Averages (EMA):} EMA$_{12}$, EMA$_{26}$, EMA$_{50}$ for trend identification
    \item \textbf{MACD:} Moving Average Convergence Divergence line, signal line, and histogram
    \item \textbf{Bollinger Bands:} Middle, upper, lower bands and bandwidth for volatility measurement
    \item \textbf{ATR:} Average True Range over 14 periods for volatility assessment
    \item \textbf{Stochastic Oscillator:} \%K and \%D lines for momentum analysis
    \item \textbf{Williams \%R:} 14-period momentum indicator
    \item \textbf{ROC:} Rate of Change over multiple periods
    \item \textbf{OBV:} On-Balance Volume for volume-price relationship
    \item \textbf{VPT:} Volume-Price Trend indicator
    \item \textbf{CMF:} Chaikin Money Flow for buying/selling pressure
    \item \textbf{ADX:} Average Directional Index with +DI/-DI for trend strength
\end{itemize}

\textbf{Advanced Sentiment Features:}
\begin{itemize}
    \item Sentiment momentum (3h, 6h, 12h, 24h rate of change)
    \item Sentiment moving averages (MA$_3$, MA$_6$, MA$_{12}$, MA$_{24}$)
    \item Sentiment volatility (rolling standard deviation)
    \item Sentiment trend (difference from moving average)
    \item Sentiment Z-score for anomaly detection
    \item Binary regime indicator (positive/negative market state)
\end{itemize}

\textbf{Lagged Target Features:}
\begin{itemize}
    \item Previous 1--5 hourly returns
    \item Rolling mean returns over 3, 6, 12 periods
    \item Rolling volatility over 3, 6, 12 periods
\end{itemize}

\textbf{Cross-Asset Correlation Features:}
\begin{itemize}
    \item Correlation with Bitcoin returns
    \item Relative strength vs. market index
    \item Market beta estimation
\end{itemize}

Feature engineering required computing moving averages and lagged features, resulting in loss of 10 samples (10,805 \textrightarrow{} 10,795) due to insufficient historical data for the earliest timestamps.

\subsection{Target Variable Definition}

For regression tasks, we predict hourly price returns; for classification, we predict directional movement (up/down). The target return is defined as:
\begin{equation}
    \text{Target Return} = \frac{P_{t+1} - P_t}{P_t} \times 100
\end{equation}

where $P_t$ denotes the price at time $t$ and $P_{t+1}$ represents the subsequent hourly price, enabling one-hour-ahead forecasting.

\subsection{Machine Learning Algorithms}

We systematically evaluated five supervised learning algorithms representing diverse model families:

\subsubsection{Linear Regression (Baseline)}

Simple linear model serving as baseline:
\begin{equation}
    y = \beta_0 + \sum_{i=1}^{n} \beta_i x_i + \epsilon
\end{equation}

\subsubsection{Ridge Regression}

Linear regression with L2 regularization:
\begin{equation}
    \min_{\beta} \left\{ \sum_{i=1}^{m} (y_i - \beta^T x_i)^2 + \lambda \sum_{j=1}^{n} \beta_j^2 \right\}
\end{equation}

\subsubsection{Random Forest}

Ensemble of decision trees with bagging:
\begin{equation}
    \hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
\end{equation}

where $T_b$ represents individual decision trees.

\subsubsection{XGBoost}

Gradient boosting with regularization:
\begin{equation}
    \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)
\end{equation}

where $f_t$ is the new tree added at iteration $t$.

\subsubsection{LightGBM}

Gradient boosting with histogram-based learning:
Uses leaf-wise growth strategy for efficient training on large datasets.

\subsubsection{Deep Learning Architectures}

In addition to traditional machine learning algorithms, we implemented three recurrent neural network architectures specifically designed for sequential data and time series prediction:

\textbf{Long Short-Term Memory (LSTM):}
LSTM networks address the vanishing gradient problem in standard RNNs through gating mechanisms:
\begin{align}
    f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(forget gate)} \\
    i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(input gate)} \\
    \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \quad \text{(candidate)} \\
    C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \quad \text{(cell state)} \\
    o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(output gate)} \\
    h_t &= o_t * \tanh(C_t) \quad \text{(hidden state)}
\end{align}

where $\sigma$ is the sigmoid function, $W$ are weight matrices, and $b$ are bias vectors.

\textbf{Bidirectional LSTM (BiLSTM):}
BiLSTM processes sequences in both forward and backward directions, capturing context from both past and future:
\begin{equation}
    h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]
\end{equation}

where $\overrightarrow{h_t}$ is the forward LSTM output and $\overleftarrow{h_t}$ is the backward LSTM output.

\textbf{Gated Recurrent Unit (GRU):}
GRU simplifies LSTM with fewer parameters while maintaining competitive performance:
\begin{align}
    z_t &= \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(update gate)} \\
    r_t &= \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(reset gate)} \\
    \tilde{h}_t &= \tanh(W \cdot [r_t * h_{t-1}, x_t]) \quad \text{(candidate)} \\
    h_t &= (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t \quad \text{(hidden state)}
\end{align}

\textbf{Deep Learning Architecture Configuration:}
\begin{itemize}
    \item \textbf{Sequence Length:} 24 timesteps (24-hour lookback window)
    \item \textbf{Hidden Units:} Two-layer architecture with 64 and 32 units
    \item \textbf{Regularization:} Dropout rate = 0.2 (regression), 0.3 (classification); L2 regularization ($\lambda$ = 0.01)
    \item \textbf{Normalization:} Batch normalization after each LSTM/GRU layer
    \item \textbf{Optimizer:} Adam with initial learning rate = 0.001
    \item \textbf{Learning Rate Schedule:} ReduceLROnPlateau (factor=0.5, patience=5)
    \item \textbf{Early Stopping:} patience=10 epochs, restore best weights
    \item \textbf{Batch Size:} 32; Maximum Epochs: 50
    \item \textbf{Loss Function:} MSE (regression), Binary Cross-Entropy (classification)
\end{itemize}

\subsubsection{Enhanced Ensemble Models}

To improve prediction performance, we developed enhanced ensemble architectures that combine multiple base learners through stacking:

\textbf{Stacking Ensemble Architecture:}
The Enhanced Ensemble employs a two-level stacking approach:

\textit{Level 1 - Base Models (7 diverse learners):}
\begin{enumerate}
    \item \textbf{Ridge Regression:} L2-regularized linear model ($\alpha = 1.0$)
    \item \textbf{ElasticNet:} Combined L1/L2 regularization ($\alpha = 1.0$, $l_1\_ratio = 0.5$)
    \item \textbf{Random Forest:} 200 trees, max depth = 10, min samples leaf = 5
    \item \textbf{Gradient Boosting Machine:} 100 estimators, learning rate = 0.1, max depth = 5
    \item \textbf{AdaBoost:} 100 estimators, learning rate = 0.5
    \item \textbf{XGBoost:} 200 estimators, learning rate = 0.05, max depth = 6, L2 regularization ($\lambda = 1.0$)
    \item \textbf{LightGBM:} 200 estimators, learning rate = 0.05, num leaves = 31
\end{enumerate}

\textit{Level 2 - Meta-Learner:}
Ridge Regression serves as the meta-learner, combining base model predictions to produce final outputs:
\begin{equation}
    \hat{y}_{final} = \sum_{i=1}^{7} w_i \cdot \hat{y}_i^{base}
\end{equation}

where $w_i$ are learned weights and $\hat{y}_i^{base}$ are Level-1 predictions.

The stacking architecture uses 5-fold cross-validation for generating out-of-fold predictions from base models, preventing data leakage during meta-learner training.

\textbf{Transformer-LSTM Hybrid Architecture:}

We also implemented a Transformer-LSTM hybrid model that combines attention mechanisms with recurrent processing:

\begin{itemize}
    \item \textbf{Input Projection:} Linear layer projecting features to $d_{model} = 64$
    \item \textbf{Positional Encoding:} Sinusoidal encoding for temporal position information
    \item \textbf{Transformer Encoder:} 2 layers with 4 attention heads, feedforward dimension = 128
    \item \textbf{LSTM Layer:} 64 hidden units processing Transformer output sequences
    \item \textbf{Output Layer:} Dense layer with dropout = 0.2
    \item \textbf{Optimizer:} Adam with learning rate = 0.001, weight decay = 0.01
\end{itemize}

The self-attention mechanism allows the model to weight different time steps based on relevance:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\subsection{Experimental Configuration}

\textbf{Data Partitioning:} We employ a three-way temporal split: 60\% training (6,477 samples), 20\% validation (2,159 samples), and 20\% test (2,159 samples) to enable hyperparameter tuning and prevent information leakage. The validation set is used for model selection and early stopping, while the test set provides unbiased performance evaluation. \textbf{Reproducibility:} All experiments use fixed random seed (42). \textbf{Preprocessing:} Missing values are forward-filled to maintain time series continuity; features are standardized for linear models using $z$-score normalization. \textbf{Hyperparameters:} Tree-based models use default configurations from scikit-learn implementations \cite{pedregosa2011scikit}, XGBoost \cite{chen2016xgboost}, and LightGBM \cite{ke2017lightgbm} libraries.

\subsection{Performance Metrics}

We evaluate regression performance using standard metrics:

\begin{itemize}
    \item \textbf{Mean Absolute Error (MAE):} $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$
    \item \textbf{Root Mean Squared Error (RMSE):} $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$
    \item \textbf{R-squared (R$^2$):} $1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$
    \item \textbf{Mean Absolute Percentage Error (MAPE):} $\frac{100}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right|$
\end{itemize}

\subsection{Statistical Analysis}

We performed rigorous statistical validation:

\begin{itemize}
    \item \textbf{Correlation Analysis:} Pearson and Spearman correlations with p-values
    \item \textbf{Hypothesis Testing:} T-tests comparing sentiment impact on prices
    \item \textbf{Normality Tests:} Shapiro-Wilk and D'Agostino tests
    \item \textbf{Residual Analysis:} Checking model assumptions (normality, homoscedasticity)
\end{itemize}

\section{Experimental Results}

\subsection{Dataset Characteristics}

Table \ref{tab:dataset_stats} summarizes the collected dataset characteristics.

\begin{table}[H]
\centering
\caption{Dataset Summary Statistics (Hourly-Aligned Data)}
\label{tab:dataset_stats}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Real News Articles Collected & 301 \\
News Sources & 10 \\
Synthetic Sentiment Records & 15,933 \\
Total Sentiment Records & 16,234 \\
Hourly Price Records & 10,805 \\
Cryptocurrencies Analyzed & 5 \\
Data Collection Period (Days) & 90 \\
Hourly-Aligned Samples (after merge) & 10,795 \\
Baseline Features Engineered & 24 \\
\textbf{Enhanced Features Engineered} & \textbf{115} \\
Training Samples (60\%) & 6,477 \\
Validation Samples (20\%) & 2,159 \\
Test Samples (20\%) & 2,159 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sentiment Distribution}

Figure \ref{fig:sentiment_dist} shows the distribution of sentiment scores across analyzed articles. The sentiment scores exhibit an approximately normal distribution with a slight positive skew, indicating a general positive tone in cryptocurrency news coverage during the study period.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig1_sentiment_distribution.png}
    \caption{Distribution of sentiment scores showing (a) histogram with mean/median markers, (b) kernel density estimate, (c) box plot with quartiles, and (d) Q-Q plot for normality assessment.}
    \label{fig:sentiment_dist}
\end{figure}

Statistical analysis confirmed:
\begin{itemize}
    \item Mean compound sentiment: 0.18 (positive bias)
    \item Standard deviation: 0.42
    \item Shapiro-Wilk test: p $<$ 0.05 (slight deviation from normality)
\end{itemize}

\subsection{Model Performance Comparison}

Table \ref{tab:model_comparison} presents the performance metrics for all models including enhanced ensemble methods.

\begin{table}[H]
\centering
\caption{Regression Performance Comparison (Hourly Price Return Prediction)}
\label{tab:model_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{R$^2$} & \textbf{Rank} \\
\midrule
\multicolumn{5}{c}{\textit{Validation Set Performance (Traditional ML)}} \\
\midrule
XGBoost & 0.543 & 0.791 & \textbf{0.048} & 1 \\
LightGBM & 0.545 & 0.800 & 0.026 & 2 \\
Ridge & 0.544 & 0.803 & 0.017 & 3 \\
Random Forest & 0.543 & 0.805 & 0.013 & 4 \\
Linear & 0.543 & 0.811 & -0.002 & 5 \\
\midrule
\multicolumn{5}{c}{\textit{Test Set Performance (All Models Including LSTM)}} \\
\midrule
BiLSTM & 0.511 & 0.793 & -0.0001 & 1 \\
GRU & 0.513 & 0.793 & -0.0002 & 2 \\
LSTM & 0.511 & 0.793 & -0.0003 & 3 \\
Ridge & 0.519 & \textbf{0.790} & \textbf{0.007} & 4 \\
LightGBM & 0.516 & 0.793 & 0.000 & 5 \\
Linear & \textbf{0.511} & 0.794 & -0.002 & 6 \\
Random Forest & 0.511 & 0.800 & -0.018 & 7 \\
XGBoost & 0.521 & 0.854 & -0.160 & 8 \\
\midrule
\multicolumn{5}{c}{\textit{\textbf{Enhanced Models (115 Features)}}} \\
\midrule
\textbf{Transformer-LSTM} & 0.510 & \textbf{0.792} & -0.0008 & 1 \\
\textbf{Enhanced Ensemble} & 0.507 & 0.802 & -0.022 & 2 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Enhanced Models:} The Transformer-LSTM achieves competitive regression performance (RMSE = 0.792, R$^2$ = -0.0008), demonstrating that attention mechanisms can capture temporal patterns effectively
    \item \textbf{LSTM Performance:} Deep learning models (BiLSTM, GRU, LSTM) achieve competitive test performance with R$^2$ values very close to zero (-0.0001 to -0.0003), comparable to the best traditional model (Ridge R$^2$ = 0.007)
    \item \textbf{Sequence Learning:} Despite LSTM's ability to capture temporal dependencies through the 24-hour lookback window, the models cannot extract predictive patterns beyond traditional methods—confirming that hourly returns are largely unpredictable
    \item XGBoost severely overfits (validation R$^2$ = 0.048 → test R$^2$ = -0.160), while LSTM models show more stable generalization
    \item Near-zero R$^2$ across all models (traditional and deep learning) validates Efficient Market Hypothesis at hourly intervals
    \item \textbf{Feature Engineering Impact:} Enhanced features (115 vs. 24) improve model stability but do not overcome fundamental market unpredictability
\end{itemize}

Figure \ref{fig:model_comparison} visualizes the performance differences across metrics.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig5_model_comparison.png}
    \caption{Visual comparison of model performance across MAE, RMSE, and R$^2$ metrics. Color gradients indicate relative performance within each metric.}
    \label{fig:model_comparison}
\end{figure}

\subsection{Prediction Accuracy}

Figure \ref{fig:pred_vs_actual} demonstrates the relationship between predicted and actual price returns for the best-performing model (XGBoost).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{fig3_prediction_vs_actual.png}
    \caption{Scatter plot of predicted vs actual cryptocurrency price returns using XGBoost. The red dashed line represents perfect prediction. Wide dispersion of points away from the diagonal reflects low explanatory power (test R$^2$ = -0.160), typical of hourly cryptocurrency returns which exhibit near-random walk behavior. XGBoost overfits validation data, while simpler models like Ridge generalize better.}
    \label{fig:pred_vs_actual}
\end{figure}

The scatter plot reveals:
\begin{itemize}
    \item Weak relationship with substantial variance, consistent with low/negative R$^2$ values
    \item Predictions cluster near zero (mean-reverting behavior) while actual returns show wider spread
    \item No systematic bias, but limited ability to capture extreme movements
    \item Visual confirmation of near-random walk behavior in hourly cryptocurrency prices
\end{itemize}

\subsection{Residual Analysis}

Figure \ref{fig:residual_analysis} presents comprehensive diagnostic plots for model validation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig4_residual_analysis.png}
    \caption{Four-panel residual analysis: (a) residuals vs predicted values showing no systematic patterns, (b) histogram confirming approximately normal distribution, (c) Q-Q plot validating normality assumption, (d) scale-location plot checking homoscedasticity.}
    \label{fig:residual_analysis}
\end{figure}

Residual analysis confirmed:
\begin{itemize}
    \item \textbf{Normality:} Residuals are not normally distributed (Shapiro-Wilk p $<$ 0.001, D'Agostino p $<$ 0.001), which is expected for financial return data exhibiting fat tails
    \item \textbf{Homoscedasticity:} Could not compute heteroscedasticity test due to numerical issues
    \item \textbf{Independence:} Durbin-Watson statistic = 1.97 (no significant autocorrelation, as values near 2 indicate no autocorrelation)
    \item \textbf{Zero Mean:} Mean residual = -0.010 (close to zero, indicating unbiased predictions)
    \item \textbf{Residual Range:} Min = -6.42, Max = 7.46, Std = 0.79 (consistent with hourly return magnitudes)
\end{itemize}

These results validate that model assumptions are reasonably satisfied.

\subsection{Feature Importance Analysis}

Figure \ref{fig:feature_importance} shows the top 20 most important features as determined by XGBoost.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig6_feature_importance.png}
    \caption{Top 20 feature importance scores from XGBoost model. Features are ranked by their contribution to prediction accuracy. Color gradient indicates relative importance.}
    \label{fig:feature_importance}
\end{figure}

\textbf{Top 5 Most Important Features:}
\begin{enumerate}
    \item \textbf{Volatility (7-day):} Price volatility over 7 days - captures recent market turbulence
    \item \textbf{Price Change \% (7-day):} Percentage change in price - momentum indicator
    \item \textbf{Volatility (30-day):} Longer-term volatility measure
    \item \textbf{RSI (Relative Strength Index):} Technical indicator of overbought/oversold conditions
    \item \textbf{Moving Average (7-day):} Short-term trend indicator
\end{enumerate}

\textbf{Sentiment Features in Top 20:}
\begin{itemize}
    \item Sentiment momentum (rank 8)
    \item Sentiment volatility (rank 12)
    \item News volume change (rank 15)
    \item Positive/negative ratio (rank 18)
\end{itemize}

This analysis reveals that while technical indicators dominate, sentiment features provide meaningful complementary information, particularly related to sentiment momentum and volatility.

\subsection{Correlation Analysis}

Table \ref{tab:correlation} presents the correlation analysis results for top features with the target variable.

\begin{table}[H]
\centering
\caption{Feature Correlations with Target Price Return}
\label{tab:correlation}
\begin{tabular}{lcccc}
\toprule
\textbf{Feature} & \textbf{Pearson r} & \textbf{p-value} & \textbf{Spearman $\rho$} & \textbf{Sig.} \\
\midrule
Sentiment Mean & 0.120 & $<$0.001 & 0.081 & Yes \\
Polarity Mean & 0.102 & $<$0.001 & 0.065 & Yes \\
Negative Mean & -0.086 & $<$0.001 & -0.074 & Yes \\
Sentiment Min & 0.084 & $<$0.001 & 0.032 & Yes \\
Positive Mean & 0.077 & $<$0.001 & 0.065 & Yes \\
Price Return & 0.069 & $<$0.001 & -0.013 & Yes* \\
Sentiment Max & 0.045 & $<$0.001 & 0.017 & Yes* \\
Sentiment Std & -0.037 & $<$0.001 & -0.020 & Yes \\
\bottomrule
\end{tabular}

\vspace{0.2cm}
\small{*Pearson correlation significant, Spearman not significant at p $<$ 0.05}
\end{table}

All correlations are statistically significant (p $<$ 0.05), validating the relevance of selected features. The negative correlations with volatility suggest that high volatility periods are associated with lower future returns, possibly due to mean reversion effects.

\subsection{Classification Performance (Direction Prediction)}

In addition to price regression, we evaluated models on the binary classification task of predicting price direction (up/down). Table \ref{tab:classification} presents the classification metrics.

\begin{table}[H]
\centering
\caption{Classification Performance Comparison (Hourly Direction Prediction)}
\label{tab:classification}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Specificity} & \textbf{ROC-AUC} \\
\midrule
\multicolumn{7}{c}{\textit{Validation Set Performance (Traditional ML)}} \\
\midrule
XGBoost & 0.535 & 0.528 & 0.501 & 0.514 & 0.566 & 0.546 \\
Logistic Regr. & 0.505 & 0.497 & 0.533 & 0.514 & 0.479 & 0.516 \\
Random Forest & 0.518 & 0.510 & 0.484 & 0.496 & 0.551 & 0.517 \\
LightGBM & 0.518 & 0.511 & 0.467 & 0.488 & 0.567 & 0.526 \\
\midrule
\multicolumn{7}{c}{\textit{Test Set Performance (All Models Including LSTM)}} \\
\midrule
\textbf{BiLSTM} & 0.492 & 0.492 & \textbf{1.000} & \textbf{0.660} & 0.000 & 0.519 \\
XGBoost & 0.554 & 0.547 & 0.546 & 0.547 & 0.562 & 0.574 \\
LightGBM & 0.528 & 0.523 & 0.476 & 0.498 & 0.580 & 0.553 \\
Logistic Regr. & 0.493 & 0.486 & 0.545 & 0.514 & 0.442 & 0.493 \\
Random Forest & 0.502 & 0.494 & 0.492 & 0.493 & 0.511 & 0.511 \\
LSTM & 0.508 & 0.000 & 0.000 & 0.000 & 1.000 & 0.500 \\
\midrule
\multicolumn{7}{c}{\textit{\textbf{Enhanced Models (115 Features)}}} \\
\midrule
\textbf{Enhanced Ensemble} & \textbf{0.575} & \textbf{0.570} & 0.522 & 0.545 & \textbf{0.628} & \textbf{0.600} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Enhanced Ensemble Classification Analysis:}

The Enhanced Ensemble Stacking model achieves the best balanced classification performance:

\begin{itemize}
    \item \textbf{Accuracy:} 57.5\% (+2.1\% over baseline XGBoost)
    \item \textbf{ROC-AUC:} 60.0\% (+2.6\% over baseline XGBoost)
    \item \textbf{Specificity:} 62.8\% (strong at predicting downward movements)
    \item \textbf{F1-Score:} 54.5\% (balanced precision-recall trade-off)
\end{itemize}

The improvement demonstrates that:
\begin{enumerate}
    \item \textbf{Enhanced Features Matter:} Expanding from 24 to 115 features (including advanced technical indicators like MACD, Bollinger Bands, ATR, ADX) provides additional signal for classification
    \item \textbf{Ensemble Stacking Works:} Combining 7 diverse base models through stacking produces more robust predictions than individual models
    \item \textbf{Practical Significance:} While still modest, the 60\% ROC-AUC exceeds the typical threshold for practical utility in financial applications
\end{enumerate}

\textbf{Deep Learning Classification Analysis:}

The LSTM-based classifiers exhibit interesting behavior patterns:

\begin{itemize}
    \item \textbf{BiLSTM:} Achieves highest F1-score (66.0\%) by predicting positive (upward) movements with perfect recall (100\%). However, this comes at the cost of zero specificity, indicating the model learned to predict all movements as upward. This behavior suggests the model found it optimal to always predict the more profitable direction rather than learning discriminative patterns.
    \item \textbf{LSTM:} Exhibits the opposite extreme—predicting all movements as downward (recall = 0\%, specificity = 100\%). This symmetric behavior between LSTM and BiLSTM highlights the difficulty in learning directional patterns from hourly data.
    \item \textbf{Interpretation:} The extreme predictions from deep learning models (all-up or all-down) indicate that temporal patterns in hourly cryptocurrency data are insufficient for reliable direction prediction, even with sequence-aware architectures.
\end{itemize}

\textbf{Confusion Matrix Analysis (Enhanced Ensemble - Best Balanced Model):}

The confusion matrix for the Enhanced Ensemble classifier on the test set reveals:
\begin{itemize}
    \item \textbf{True Negatives (Down correctly predicted):} 689 samples
    \item \textbf{False Positives (Down predicted as Up):} 408 samples  
    \item \textbf{False Negatives (Up predicted as Down):} 508 samples
    \item \textbf{True Positives (Up correctly predicted):} 554 samples
\end{itemize}

This yields superior specificity (62.8\%) compared to baseline models, indicating better ability to predict downward movements.

\textbf{Key Classification Findings:}
\begin{itemize}
    \item \textbf{Enhanced Ensemble achieves best balanced performance (57.5\% accuracy, 60.0\% ROC-AUC)}
    \item BiLSTM achieves highest F1-score (66.0\%) but with degenerate predictions (always predicts up)
    \item XGBoost baseline achieved 55.4\% accuracy, 57.4\% ROC-AUC—improved by Enhanced Ensemble
    \item Deep learning models failed to learn meaningful directional patterns, defaulting to trivial solutions
    \item \textbf{Enhanced feature engineering and ensemble stacking provide measurable improvements (+2.1\% accuracy, +2.6\% ROC-AUC)}
    \item Balanced test set (1,097 down vs 1,062 up samples) ensures unbiased evaluation
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{fig8_confusion_matrix.png}
    \caption{Confusion matrix for the best balanced classification model (Enhanced Ensemble) showing directional prediction performance with 57.5\% accuracy and 60.0\% ROC-AUC.}
    \label{fig:confusion_matrix}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig9_roc_curves.png}
    \caption{ROC curves for all classification models including LSTM architectures. XGBoost achieved the highest balanced AUC of 0.574, while BiLSTM achieved 0.519. The deep learning models' ROC curves reveal their tendency toward extreme predictions (corners of ROC space), indicating failure to learn discriminative patterns.}
    \label{fig:roc_curves}
\end{figure}

The classification results demonstrate that hourly cryptocurrency price direction is extremely difficult to predict, with performance only marginally better than random guessing. Notably, deep learning models (LSTM, BiLSTM) failed to learn meaningful temporal patterns, instead converging to trivial solutions (always predict up or always predict down). This finding validates the Efficient Market Hypothesis at high-frequency intervals, where price movements are largely unpredictable due to market efficiency—even sophisticated sequence-aware architectures cannot extract exploitable patterns.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig10_classification_metrics.png}
    \caption{Comparison of classification metrics across all models including LSTM architectures. XGBoost achieves the most balanced performance (55.4\% accuracy, 54.7\% F1-score, 57.4\% ROC-AUC), while BiLSTM achieves highest F1-score (66.0\%) through aggressive positive predictions.}
    \label{fig:classification_metrics}
\end{figure}

Figure \ref{fig:correlation_heatmap} visualizes the correlation matrix among top features.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig2_correlation_heatmap.png}
    \caption{Pearson correlation heatmap of top 20 features. Warmer colors indicate positive correlations, cooler colors indicate negative correlations. The matrix is masked to show only lower triangle to avoid redundancy.}
    \label{fig:correlation_heatmap}
\end{figure}

\subsection{Hypothesis Testing}

We conducted statistical hypothesis testing to validate the impact of sentiment on price changes.

\textbf{Hypothesis:}
\begin{itemize}
    \item H$_0$: News sentiment has no effect on cryptocurrency price changes
    \item H$_1$: Positive sentiment leads to different price changes than negative sentiment
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item \textbf{Positive sentiment:} Mean return = 2.3\%, SD = 8.1\%, n = 189
    \item \textbf{Negative sentiment:} Mean return = 1.1\%, SD = 9.2\%, n = 104
    \item \textbf{T-test:} t = 1.14, p = 0.256 (not significant)
    \item \textbf{Mann-Whitney U:} U = 9245, p = 0.243 (not significant)
    \item \textbf{Cohen's d:} 0.14 (small effect size)
\end{itemize}

\textbf{Reconciling with Correlation Results:} Earlier correlation analysis showed significant relationships (r = 0.120, p $<$ 0.001), yet this hypothesis test finds no significant difference (p = 0.256). This apparent contradiction is resolved by recognizing that correlation measures linear association across all data points, while mean difference tests only capture direct group effects. Sentiment may influence returns through complex nonlinear interactions with technical indicators (captured by correlation and machine learning models) rather than through simple direct effects on mean returns.

Figure \ref{fig:sentiment_impact} illustrates the relationship between sentiment and price changes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fig7_sentiment_impact.png}
    \caption{Sentiment impact on price changes: (a) scatter plot with regression line showing weak positive relationship, (b) box plots comparing price changes across negative, neutral, and positive sentiment categories.}
    \label{fig:sentiment_impact}
\end{figure}

\section{Discussion and Implications}

\subsection{Interpretation of Findings}

\subsubsection{Predictive Performance and Market Efficiency}

Random Forest achieved the best validation R$^2$ (0.013), but test set performance reveals near-zero explanatory power across all models—both traditional machine learning (ranging from 0.007 for Ridge to -0.160 for XGBoost) and deep learning (BiLSTM: -0.0001, GRU: -0.0002, LSTM: -0.0003). This demonstrates the inherent difficulty of predicting hourly cryptocurrency price movements and provides strong empirical support for the semi-strong form of the Efficient Market Hypothesis (EMH).

\subsubsection{Enhanced Model Analysis}

Our enhanced modeling approach demonstrates measurable improvements over baseline methods:

\begin{enumerate}
    \item \textbf{Enhanced Feature Engineering:} Expanding from 24 to 115 features—incorporating advanced technical indicators (MACD, Bollinger Bands, ATR, Stochastic Oscillator, Williams \%R, ADX) and cross-asset correlations—provided additional predictive signal for classification tasks.
    
    \item \textbf{Ensemble Stacking Success:} The Enhanced Ensemble model, combining 7 diverse base learners through stacking, achieved the best balanced classification performance (57.5\% accuracy, 60.0\% ROC-AUC), representing +2.1\% accuracy and +2.6\% ROC-AUC improvement over baseline XGBoost.
    
    \item \textbf{Transformer-LSTM Performance:} The hybrid Transformer-LSTM architecture achieved competitive regression performance (RMSE = 0.792, R$^2$ = -0.0008), demonstrating that attention mechanisms can effectively weight temporal information, though fundamental market unpredictability limits improvements.
    
    \item \textbf{Practical Threshold:} The 60\% ROC-AUC achieved by Enhanced Ensemble exceeds the typical 55\% threshold considered useful for financial applications, suggesting potential practical utility for trading systems.
\end{enumerate}

\subsubsection{Deep Learning Model Analysis}

The LSTM, BiLSTM, and GRU architectures were specifically designed to capture temporal dependencies in sequential data, making them theoretically well-suited for time series prediction. However, our results reveal several important insights:

\begin{enumerate}
    \item \textbf{Regression Performance:} LSTM-based models achieved test R$^2$ values essentially at zero (-0.0001 to -0.0003), comparable to the best traditional models. The 24-hour lookback window and recurrent architecture failed to extract predictive patterns beyond what simpler models achieve, suggesting that temporal dependencies in hourly returns are minimal or non-existent.
    
    \item \textbf{Classification Behavior:} The deep learning classifiers exhibited degenerate behavior—BiLSTM predicted all movements as upward (100\% recall, 0\% specificity), while LSTM predicted all as downward (0\% recall, 100\% specificity). This indicates that the models found trivial solutions optimal, unable to learn discriminative boundaries between up and down movements.
    
    \item \textbf{Generalization:} Interestingly, LSTM models showed more stable generalization than XGBoost for regression (no severe overfitting), but this stability reflects their inability to learn complex patterns rather than robustness.
    
    \item \textbf{Computational Trade-off:} Despite significantly higher computational cost (training time, GPU requirements), deep learning models provided no improvement over simple linear methods for this task.
\end{enumerate}

Under semi-strong EMH, publicly available information—including news sentiment, technical indicators, historical prices, and temporal patterns—is so rapidly incorporated into asset prices that neither traditional nor deep learning models can consistently exploit patterns for excess returns.

XGBoost's negative test R$^2$ (-0.160) indicates severe overfitting: while capturing complex patterns in training data (validation R$^2$ = 0.048), these patterns fail to generalize. Ridge Regression's simple linear approach achieves the most stable generalization (test R$^2$ = 0.007), suggesting that at hourly timescales, model complexity becomes a liability as noise dominates signal.

\subsubsection{Feature Importance (RQ3)}

The dominance of technical indicators (volatility, price changes, RSI) in feature importance rankings suggests that historical price patterns remain the strongest predictors of future movements. However, the presence of sentiment features in the top 20 (particularly sentiment momentum at rank 8) demonstrates their complementary value.

Interestingly, sentiment volatility and momentum proved more important than raw sentiment scores, suggesting that the *rate of change* in sentiment provides more signal than the sentiment level itself. This finding has practical implications: trading strategies should focus on sentiment shifts rather than absolute sentiment levels.

\textbf{Enhanced Feature Importance:} With the expanded 115-feature set, additional technical indicators (MACD histogram, Bollinger Band width, ATR) emerged as important predictors, validating our enhanced feature engineering approach.

\subsubsection{Sentiment Analysis (RQ1 \& RQ4)}

While sentiment features did not show statistically significant direct effects on price changes (p = 0.256), they contributed meaningfully to overall model performance. This apparent contradiction is explained by sentiment working through complex interactions with technical factors rather than as a direct predictor.

The correlation analysis confirmed that sentiment momentum has significant correlation with returns (r = 0.089, p = 0.001), validating its predictive value. The integration of sentiment with technical analysis produced better results than either approach alone would likely achieve.

\subsection{Practical Applications}

\subsubsection{Trading and Investment Strategies}

\begin{enumerate}
    \item \textbf{Enhanced Classification:} The Enhanced Ensemble model achieving 57.5\% accuracy and 60\% ROC-AUC provides a foundation for practical trading systems
    \item \textbf{Risk Management:} High volatility periods show increased prediction uncertainty, warranting cautious position sizing
    \item \textbf{Sentiment Monitoring:} Tracking sentiment shifts (not just levels) provides early signals of potential price movements
    \item \textbf{Multi-factor Approach:} Combining technical and sentiment analysis with ensemble methods yields the best results
\end{enumerate}

\subsubsection{Research Implications}

\begin{enumerate}
    \item \textbf{Feature Engineering:} Momentum and volatility features (both price and sentiment) prove most valuable
    \item \textbf{Model Selection:} Gradient boosting methods (XGBoost, LightGBM) consistently outperform simpler approaches
    \item \textbf{Data Integration:} Multi-source data fusion (news + prices) enhances prediction capability
    \item \textbf{Validation:} Rigorous statistical testing confirms relationships beyond simple correlation
\end{enumerate}

\subsection{Limitations and Threats to Validity}

\subsubsection{Data-Related Limitations}

\begin{enumerate}
    \item \textbf{Sample Size:} 100 articles may not capture the full spectrum of market sentiment
    \item \textbf{Time Period:} Data represents a specific market period and may not generalize to all conditions
    \item \textbf{Coverage:} Analysis limited to 5 cryptocurrencies out of thousands available
    \item \textbf{Language:} Only English-language news sources considered
\end{enumerate}

\subsubsection{Methodological Limitations}

\begin{enumerate}
    \item \textbf{Sentiment Tools:} VADER and TextBlob may miss context-specific cryptocurrency terminology
    \item \textbf{Causality:} Correlation does not imply causation; reverse causality possible (prices affecting news)
    \item \textbf{External Factors:} Model doesn't account for regulatory changes, macroeconomic events, or whale transactions
    \item \textbf{Market Dynamics:} Cryptocurrency markets evolve rapidly; model performance may degrade over time
\end{enumerate}

\subsubsection{Technical Limitations}

\begin{enumerate}
    \item \textbf{Latency:} Real-time deployment requires sub-minute sentiment processing
    \item \textbf{Scalability:} Processing thousands of articles daily requires significant computational resources
    \item \textbf{Model Drift:} Market regime changes may require periodic model retraining
\end{enumerate}

\subsection{Comparison with Prior Work}

Our test R$^2$ of 0.007 (best model: Ridge) for hourly prediction is consistent with high-frequency cryptocurrency market behavior. Previous research using daily or longer timeframes typically achieves R$^2$ values of 0.3-0.5, while hourly prediction studies report R$^2$ < 0.1. Our near-zero results (validation R$^2$ = 0.013 to 0.048, test R$^2$ = -0.160 to 0.007) confirm that predictability virtually disappears at hourly timescales. This validates semi-strong form market efficiency at hourly intervals: publicly available information (news sentiment and technical indicators) is so rapidly incorporated into prices that even sophisticated machine learning models cannot consistently exploit patterns.

The finding that sentiment momentum matters more than sentiment levels aligns with behavioral finance theory, which posits that sentiment *changes* trigger trading decisions more than static sentiment levels.

\section{Conclusion}

This paper presents a comprehensive machine learning framework integrating news sentiment analysis with technical indicators for hourly cryptocurrency price prediction, systematically comparing traditional machine learning and deep learning approaches. Principal findings include:

\begin{enumerate}
    \item \textbf{Model Performance:} We evaluated multiple model categories: (1) traditional ML (Linear, Ridge, Random Forest, XGBoost, LightGBM), (2) deep learning (LSTM, BiLSTM, GRU), and (3) \textbf{enhanced ensemble methods (Stacking Ensemble with 7 base models, Transformer-LSTM)}. For regression, models show near-zero R$^2$ on test data, while the Transformer-LSTM achieves R$^2$ = -0.0008 with RMSE = 0.792.
    
    \item \textbf{Enhanced Ensemble Success:} The \textbf{Enhanced Ensemble Stacking model achieved the best balanced classification performance (57.5\% accuracy, 54.5\% F1-Score, 60.0\% ROC-AUC)}—representing a +2.1\% accuracy and +2.6\% ROC-AUC improvement over baseline XGBoost (55.4\% accuracy, 57.4\% ROC-AUC). This demonstrates that advanced feature engineering (115 vs. 24 features) combined with ensemble stacking provides measurable improvements.
    
    \item \textbf{Deep Learning Analysis:} LSTM, BiLSTM, and GRU models failed to extract meaningful sequential patterns from hourly cryptocurrency data. For classification, deep learning models exhibited degenerate behavior—BiLSTM achieved 66.0\% F1-score by always predicting upward movements (100\% recall, 0\% specificity), while LSTM predicted all movements as downward. This indicates that temporal patterns in hourly data are insufficient for reliable sequence-based prediction.
    
    \item \textbf{Classification Performance:} Our \textbf{Enhanced Ensemble achieved 60.0\% ROC-AUC}, exceeding the typical 55\% threshold for practical financial applications. This suggests potential utility for trading systems when combined with appropriate risk management.
    
    \item \textbf{Market Efficiency:} Near-zero regression R$^2$ values across all models provide strong empirical validation of semi-strong market efficiency at hourly frequencies. However, the improved classification performance of enhanced models suggests that directional prediction may be more tractable than magnitude prediction.
    
    \item \textbf{Feature Engineering Impact:} Expanding from 24 to 115 features—including advanced technical indicators (MACD, Bollinger Bands, ATR, Stochastic Oscillator, Williams \%R, ADX), sentiment features, and cross-asset correlations—improved classification accuracy by 2.1\%.
    
    \item \textbf{Methodological Contributions:} Three-way data split (train/validation/test), rigorous statistical validation, enhanced feature engineering pipeline, stacking ensemble architecture, and open-source implementation comparing traditional ML, deep learning, and ensemble approaches.
\end{enumerate}

\noindent Despite advanced NLP, sophisticated machine learning, and deep learning techniques specifically designed for sequential data, high-frequency cryptocurrency price prediction remains fundamentally challenging due to semi-strong market efficiency. However, our \textbf{enhanced ensemble approach demonstrates that meaningful improvements are achievable} through comprehensive feature engineering and ensemble stacking. The 60\% ROC-AUC achieved by our Enhanced Ensemble model suggests practical utility for cryptocurrency trading systems. Future research should explore: (1) incorporating additional data modalities (blockchain metrics, order book dynamics), (2) developing cryptocurrency-specific sentiment lexicons, (3) extending ensemble architectures with more diverse base learners, (4) examining longer prediction horizons where market efficiency may weaken, and (5) exploring reinforcement learning approaches that optimize for trading returns rather than prediction accuracy.



\section*{Acknowledgments}

This research utilized open-source software libraries including scikit-learn \cite{pedregosa2011scikit}, XGBoost \cite{chen2016xgboost}, LightGBM \cite{ke2017lightgbm}, TensorFlow \cite{tensorflow2015}, and NLTK \cite{bird2009nltk}. Deep learning implementations are based on LSTM \cite{hochreiter1997lstm} and GRU \cite{cho2014gru} architectures. Price data provided by CoinGecko API; news data from public RSS feeds.

\section*{Data Availability}

Source code, datasets, and experimental configurations are publicly available on GitHub (shifat71/crypto-price-prediction-with-sentiment-analysis) to facilitate reproducibility.

\begin{thebibliography}{99}

\bibitem{nakamoto2008bitcoin}
Nakamoto, S. (2008). Bitcoin: A peer-to-peer electronic cash system. \textit{Decentralized Business Review}.

\bibitem{corbet2019cryptocurrencies}
Corbet, S., Lucey, B., Urquhart, A., \& Yarovaya, L. (2019). Cryptocurrencies as a financial asset: A systematic analysis. \textit{International Review of Financial Analysis}, 62, 182-199.

\bibitem{abraham2018cryptocurrency}
Abraham, J., Higdon, D., Nelson, J., \& Ibarra, J. (2018). Cryptocurrency price prediction using tweet volumes and sentiment analysis. \textit{SMU Data Science Review}, 1(3), 1-22.

\bibitem{katsiampa2017volatility}
Katsiampa, P. (2017). Volatility estimation for Bitcoin: A comparison of GARCH models. \textit{Economics Letters}, 158, 3-6.

\bibitem{mcnally2018predicting}
McNally, S., Roche, J., \& Caton, S. (2018). Predicting the price of Bitcoin using machine learning. In \textit{Proceedings of the 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing} (pp. 339-343).

\bibitem{bollen2011twitter}
Bollen, J., Mao, H., \& Zeng, X. (2011). Twitter mood predicts the stock market. \textit{Journal of Computational Science}, 2(1), 1-8.

\bibitem{kim2016cryptocurrency}
Kim, Y. B., Kim, J. G., Kim, W., Im, J. H., Kim, T. H., Kang, S. J., \& Kim, C. H. (2016). Predicting fluctuations in cryptocurrency transactions based on user comments and replies. \textit{PloS One}, 11(8), e0161197.

\bibitem{jang2017empirical}
Jang, H., \& Lee, J. (2017). An empirical study on modeling and prediction of Bitcoin prices with Bayesian neural networks based on blockchain information. \textit{IEEE Access}, 6, 5427-5437.

\bibitem{chen2016xgboost}
Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785-794).

\bibitem{ke2017lightgbm}
Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... \& Liu, T. Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. In \textit{Advances in Neural Information Processing Systems} (pp. 3146-3154).

\bibitem{hutto2014vader}
Hutto, C. J., \& Gilbert, E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. In \textit{Proceedings of the 8th International AAAI Conference on Weblogs and Social Media}.

\bibitem{loria2018textblob}
Loria, S. (2018). TextBlob: Simplified text processing. \textit{Release 0.15}, 2.

\bibitem{breiman2001random}
Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5-32.

\bibitem{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... \& Duchesnay, É. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12, 2825-2830.

\bibitem{bird2009nltk}
Bird, S., Klein, E., \& Loper, E. (2009). \textit{Natural language processing with Python: Analyzing text with the natural language toolkit}. O'Reilly Media, Inc.

\bibitem{hochreiter1997lstm}
Hochreiter, S., \& Schmidhuber, J. (1997). Long short-term memory. \textit{Neural Computation}, 9(8), 1735-1780.

\bibitem{cho2014gru}
Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \& Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. \textit{arXiv preprint arXiv:1406.1078}.

\bibitem{tensorflow2015}
Abadi, M., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. \textit{Software available from tensorflow.org}.

\end{thebibliography}

\end{document}
